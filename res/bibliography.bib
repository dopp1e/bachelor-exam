@misc{CAIncludedCertificates,
  title = {{{CA}}/{{Included Certificates}} - {{MozillaWiki}}},
  urldate = {2026-01-07},
  url = {https://wiki.mozilla.org/CA/Included_Certificates},
  file = {/home/doppie/Zotero/storage/JBRNLZ75/Included_Certificates.html}
}

@techreport{fieldingHypertextTransferProtocol2014,
  type = {Request for {{Comments}}},
  title = {Hypertext {{Transfer Protocol}} ({{HTTP}}/1.1): {{Semantics}} and {{Content}}},
  shorttitle = {Hypertext {{Transfer Protocol}} ({{HTTP}}/1.1)},
  author = {Fielding, Roy T. and Reschke, Julian},
  year = 2014,
  month = jun,
  number = {RFC 7231},
  institution = {Internet Engineering Task Force},
  doi = {10.17487/RFC7231},
  urldate = {2026-01-09},
  abstract = {The Hypertext Transfer Protocol (HTTP) is a stateless \textbackslash textbackslash\%application- level protocol for distributed, collaborative, hypertext information systems. This document defines the semantics of HTTP/1.1 messages, as expressed by request methods, request header fields, response status codes, and response header fields, along with the payload of messages (metadata and body content) and mechanisms for content negotiation.},
  file = {/home/doppie/Zotero/storage/HLMYF3PE/Fielding and Reschke - 2014 - Hypertext Transfer Protocol (HTTP1.1) Semantics and Content.pdf}
 
@article{HebbianTheory2025,
  title = {Hebbian Theory},
  year = 2025,
  month = dec,
  journal = {Wikipedia},
  urldate = {2026-01-09},
  abstract = {Hebbian theory is a neuropsychological theory claiming that an increase in synaptic efficacy arises from a presynaptic cell's repeated and persistent stimulation of a postsynaptic cell. It is an attempt to explain synaptic plasticity, the adaptation of neurons during the learning process. Hebbian theory was introduced by Donald Hebb in his 1949 book The Organization of Behavior. The theory is also called Hebb's rule, Hebb's postulate, and cell assembly theory. Hebb states it as follows: Let us assume that the persistence or repetition of a reverberatory activity (or "trace") tends to induce lasting cellular changes that add to its stability. ... When an axon of cell A is near enough to excite a cell B and repeatedly or persistently takes part in firing it, some growth process or metabolic change takes place in one or both cells such that A's efficiency, as one of the cells firing B, is increased. The theory is often summarized as "Neurons that fire together, wire together." However, Hebb emphasized that cell A needs to "take part in firing" cell B, and such causality can occur only if cell A fires just before, not at the same time as, cell B. This aspect of causation in Hebb's work foreshadowed what is now known about spike-timing-dependent plasticity, which requires temporal precedence. Hebbian theory attempts to explain associative or Hebbian learning, in which simultaneous activation of cells leads to pronounced increases in synaptic strength between those cells. It also provides a biological basis for errorless learning methods for education and memory rehabilitation. In the study of neural networks in cognitive function, it is often regarded as the neuronal basis of unsupervised learning.},
  copyright = {Creative Commons Attribution-ShareAlike License},
  langid = {english},
  annotation = {Page Version ID: 1328793004},
  url = {https://en.wikipedia.org/w/index.php?title=Hebbian_theory&oldid=1328793004}
}

@article{MNISTDatabase2025,
  title = {{{MNIST}} Database},
  year = 2025,
  month = dec,
  journal = {Wikipedia},
  urldate = {2026-01-09},
  abstract = {The MNIST database (Modified National Institute of Standards and Technology database) is a large database of handwritten digits that is commonly used for training various image processing systems. The database is also widely used for training and testing in the field of machine learning. It was created by "re-mixing" the samples from NIST's original datasets. The creators felt that since NIST's training dataset was taken from American Census Bureau employees, while the testing dataset was taken from American high school students, it was not well-suited for machine learning experiments. Furthermore, the black and white images from NIST were normalized to fit into a 28x28 pixel bounding box and anti-aliased, which introduced grayscale levels. The MNIST database contains 60,000 training images and 10,000 testing images. Half of the training set and half of the test set were taken from NIST's training dataset, while the other half of the training set and the other half of the test set were taken from NIST's testing dataset. The original creators of the database keep a list of some of the methods tested on it. In their original paper, they use a support-vector machine to get an error rate of 0.8\%. The original MNIST dataset contains at least 4 wrong labels.},
  copyright = {Creative Commons Attribution-ShareAlike License},
  langid = {english},
  annotation = {Page Version ID: 1327868530},
  url = {https://en.wikipedia.org/w/index.php?title=MNIST_database&oldid=1327868530}
}